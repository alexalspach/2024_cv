


<h2>Blog Posts<small> </small></h2>

<div id="toc-table" style="margin-left:20px; margin-right: 20px;">
  <table>

    <tr>
      <td class="table-title year">2025</td>
      <td class="table-links">

        <ul>
          <li><span><a target="_blank" href="https://bostondynamics.com/blog/large-behavior-models-atlas-find-new-footing/" title="Large Behavior Models and Atlas Find New Footing">
          Large Behavior Models and Atlas Find New Footing (Boston Dynamics)</a></span></li>
        </ul>

      </td>
    </tr> 

    <tr>
      <td class="table-title year">2024</td>
      <td class="table-links">

        <ul>
          <li><span><a target="_blank" href="https://medium.com/toyotaresearch/meet-punyo-tris-soft-robot-for-whole-body-manipulation-research-949c934ac3d8" title="MeetPunyo">
          Meet Punyo, TRI’s Soft Robot for Whole-Body Manipulation Research (Medium)</a></span></li>
        </ul>
        
      </td>
    </tr> 

  </table>
</div>

<!--
<div id="toc-table" style="margin-left:20px; margin-right: 20px;">
  <table>


   <tr>
    <td class="table-title year">2024</td>
    <td class="table-links"
    style="
      font-size: 20px;
      width: 67%;
      line-height: 25px;
      padding-top: 1.5px;
      float: left;
    ">

      <ul>

        <li><span><a target="_blank" href="https://medium.com/toyotaresearch/meet-punyo-tris-soft-robot-for-whole-body-manipulation-research-949c934ac3d8" title="MeetPunyo">
        Meet Punyo, TRI’s Soft Robot for Whole-Body Manipulation Research (Medium)</a></span></li>


        <a target="_blank" href="https://medium.com/toyotaresearch/meet-punyo-tris-soft-robot-for-whole-body-manipulation-research-949c934ac3d8"> 
          <img loading="lazy" class="media-shadow hvr-grow-rotate" 
          style="
            top: 45px;
            right: 140px;
            width: 210px;
            position: absolute;
          " src="../assets/images/projects/Punyo/Punyo2Mission2embroidered_rotated.png" alt="Early Punyo Concept Art">
        </a>


      </ul>

    </td>
  </tr> 

</table> 

</div>  

-->
  




<!-- PUBLICATIONS -->
<br>
<h2>Journal Publications<small> </small></h2>

<div id="toc-table" style="margin-left:20px; margin-right: 20px;">
  <table>


   <tr>
    <td class="table-title year">2025</td>
    <td class="table-links">

      <ul>

        <li><span><a href="#LBM1paper" title="LBM1paper">
        A Careful Examination of Large Behavior Models for Multitask Dexterous Manipulation (TBA)</a></span></li>

        <li><span><a href="#EGRL" title="EGRL">
        Learning Contact-Rich Whole-Body Manipulation with Example-Guided Reinforcement Learning (Science Robotics)</a></span></li>

      </ul>
    </td>
  </tr> 



   <tr>
    <td class="table-title year">2024</td>
    <td class="table-links">

      <ul>

        <li><span><a href="#Grip" title="Grip">
        Don't Lose Your Grip: Enhanced Grasp and Contact Estimation with a Passive Particle Jamming Visuotactile End Effector (TBA)</a></span></li>

        <li><span><a href="#Visuomotor" title="Visuomotor">
        Exploring Locally Linear Representations for Visuomotor and Tactile Control (RA-L, ICRA)</a></span></li>

      </ul>

    </td>
  </tr> 

  <tr>
    <td class="table-title year">2019</td>
    <td class="table-links">

      <ul>

        <li><span><a href="#PatchContact" title="PatchContact">
          Fast Model-Based Contact Patch and Pose Estimation for Highly Deformable Dense-Geometry Tactile Sensors (RA-L, ICRA)
          <i class="fa fa-trophy" aria-hidden="true"></i></a></span></li>

          <li><span><a href="#SmoothContact" title="SmoothContact">
            A Transition-Aware Method for the Simulation of Compliant Contact with Regularized Friction (RA-L, ICRA)
          </a></span></li>

        </ul>
        
      </td>
    </tr> 


    <tr>
      <td class="table-title year">2018</td>
      <td class="table-links">

        <ul>

          <li><span><a href="#CompDes2" title="CompDes2">
            Computational Design of Robotic Devices from High-Level Motion Specifications (T-RO)
          </a></span></li>

          <li><span><a href="#CompDes1" title="CompDes1">
            Computational Co-Optimization of Design Parameters and Motion Trajectories for Robotic Systems (IJRR)
          </a></span></li>

        </ul>

      </td>
    </tr> 

  </table>
</div>  
        



<!-- <h2>Thesis and Publications<small> </small></h2> -->
<br>
<h2>Conference Publications<small> </small></h2>




<div id="toc-table" style="margin-left:20px; margin-right: 20px;">
  <table>


    <tr>
      <td class="table-title year">2025</td>
      <td class="table-links">

        <ul>
          <li><span><a href="#PAPRLE" title="PAPRLE">
            PAPRLE (Plug-And-Play Robotic Limb Environment): A Modular Ecosystem for Robotic Limbs
          </a></span></li>
        </ul>

      </td>
    </tr>



    <tr>
      <td class="table-title year">2022</td>
      <td class="table-links">

        <ul>

          <li><span><a href="#SEED" title="SEED">
            SEED: Series Elastic End Effectors in 6D for Visuotactile Tool Use (IROS)
          </a></span></li>

          <li><span><a href="#Punyo1" title="Punyo1">
            Punyo-1: Soft Tactile-Sensing Upper-Body Robot for Large Object Manipulation<br>and Physical Human Interaction (RoboSoft)
          </a></span></li>

        </ul>

      </td>
    </tr>


    <tr>
      <td class="table-title year">2021</td>
      <td class="table-links">

        <ul>

          <li><span><a href="#ActivePressure1" title="ActivePressure1">
            Variable Compliance and Geometry Regulation of Soft-Bubble Grippers <br>with Active Pressure Control (RoboSoft)
          </a></span></li>

          <li><span><a href="#Monocular1" title="Monocular1">
            Monocular Depth Estimation for Soft Visuotactile Sensors (RoboSoft)
          </a></span></li>

        </ul>

      </td>
    </tr>   

    <tr>
      <td class="table-title year">2020</td>
      <td class="table-links">

        <ul>

          <li><span><a href="#BubbleGripper1" title="BubbleGripper1">
            Soft-Bubble Grippers for Robust and Perceptive Manipulation (IROS)
          </a></span></li>

        </ul>
        
      </td>
    </tr>   


    <tr>
      <td class="table-title year">2019</td>
      <td class="table-links">

        <ul>

          <li><span><a href="#SoftBubble" title="SoftBubble">
            Soft-bubble: A highly compliant dense geometry tactile sensor for robot manipulation (RoboSoft)
          </a></span></li>

        </ul>

      </td>
    </tr>      

    <tr>
      <td class="table-title year">2018</td>
      <td class="table-links">

        <ul>

          <li><span><a href="#SoftArm" title="SoftArm">
            Design and Fabrication of a Soft Robotic Hand and Arm System (RoboSoft)
          </a></span></li>

        </ul>

      </td>
    </tr> 

    <tr>
      <td class="table-title year">2017</td>
      <td class="table-links">

        <ul>

          <li><span><a href="#Snapbot1" title="Snapbot1">
            Snapbot: A Reconfigurable Legged Robot (IROS)
          </a></span></li>

          <li><span><a href="#ImpFuncTheor" title="ImpFuncTheor">
            Joint Optimization of Robot Design and Motion Parameters using the Implicit Function Theorem (RSS)
            <i class="fa fa-trophy" aria-hidden="true"></i></a></span></li>

        </ul>

      </td>
    </tr> 


    <tr>
      <td class="table-title year">2016</td>
      <td class="table-links">

        <ul>

          <li><span><a href="#LimbOptPaper" title="LimbOptPaper">
            Task-Based Limb Optimization for Legged Robots (IROS)
          </a></span></li>

          <li><span><a href="#SoftUkelelePaper" title="SoftUkelelePaper">
            Mechanical Implementation of a Variable-Stiffness Actuator for a Softly Strummed Ukulele (IROS)
          </a></span></li>

          <li><span><a href="#HuggingPaper" title="HuggingPaper">
            Study of Children's Hugging for Interactive Robot Design (RO-MAN)
          </a></span></li>

          <li><span><a href="#TeleHeadPaper" title="TeleHeadPaper">
            Imitating Human Movement with Teleoperated Robotic Head (RO-MAN)
            <i class="fa fa-trophy" aria-hidden="true"></i></a></span></li>

        </ul>

      </td>
    </tr> 

    <tr>
      <td class="table-title year">2015</td>
      <td class="table-links">

        <ul>

          <li><span><a href="#SoftUpperPaper" title="SoftUpperPaper">
            Design of a Soft Upper Body Robot for Physical Human-Robot Interaction (Humanoids)
            <i class="fa fa-trophy" aria-hidden="true"></i></a></span></li>

            <li><span><a href="#SkinShape" title="SkinShape">
              Analyzing Muscle Activity and Force with Skin Shape Captured by Non-contact Visual Sensor (PSIVT)
            </a></span></li>

            <li><span><a href="#SoftSkinPaper" title="SoftSkinPaper">
              3D Printed Soft Skin for Safe Human-Robot Interaction (IROS)
            </a></span></li>

        </ul>

      </td>
    </tr> 


    <tr>
      <td class="table-title year">2012</td>
      <td class="table-links">

        <ul>

          <li><span><a href="#HumanoidRobotPushing" title="HumanoidRobotPushing">
            Controlling and Maximizing the Humanoid Robot Pushing Force by Postures (URAI)
          </a></span></li>

          <li><span><a href="#HumanoidRobotPushingThesis" title="HumanoidRobotPushingThesis">
            A Humanoid Robot Pushing Model Inspired by Human Motion (Master's Thesis)
          </a></span></li>

        </ul>

      </td>
    </tr>  

    <tr>
      <td class="table-title year">2010</td>
      <td class="table-links">

        <ul>

          <li><span><a href="#CommonInterface" title="CommonInterface">
            A Common Interface for Humanoid Simulation and Hardware (IEEE-RAS)
          </a></span></li>

        </ul>

      </td>
    </tr>  

  </table>
</div>  




<br>
<h2>Thesis<small> </small></h2>


<div id="toc-table" style="margin-left:20px; margin-right: 20px;">
  <table>

    <tr>
      <td class="table-title year">2012</td>
      <td class="table-links">

        <ul>

          <li><span><a href="#HumanoidRobotPushingThesis" title="HumanoidRobotPushingThesis">
            A Humanoid Robot Pushing Model Inspired by Human Motion (Master's Thesis)
          </a></span></li>

        </ul>

      </td>
    </tr> 

  </table> 

</div>  







<br>
<h2>Book Chapters<small> </small></h2>




<div id="toc-table" style="margin-left:20px; margin-right: 20px;">
  <table>


   <tr>
    <td class="table-title year">2024</td>
    <td class="table-links">

      <ul>


        <li><span><a href="#OANeeds" title="OANeeds">
        Uncovering Older Adult Needs – Applying User-Centered Research Methodologies to Inform Robotics Development and a Call to Action (Human-Robot Interaction - A Multidisciplinary Overview, IntechOpen)</a></span></li>

      </ul>

    </td>
  </tr> 

</table> 

</div>  
  







<br>
<h2>Granted Patents<small> </small></h2>


<!-- https://www.freepatentsonline.com/result.html?p=1&srch=ezsrch&search=Search&pn=&apn=&all=&ttl=&abst=&aclm=&spec=&apd=&apdto=&isd=&isdto=&prir=&ccl=&icl=&in=ALEXANDER+ALSPACH&icn=&is=&ic=&an=&acn=&as=&ac=&ref=&fref=&oref=&parn=&pex=&asex=&agt=&uspat=on&date_range=all&stemming=on&sort=chron
-->

<div id="toc-table" style="margin-left:20px; margin-right: 20px;">
  <table>

    <tr>
      <td class="table-title year">2025</td>
      <td class="table-links">

        <ul>

          <li><span><a target="_blank" href="https://www.freepatentsonline.com/12311529.html">
             Systems and devices for surface slip detection (US12311529)
          </a></span></li> 

          <li><span><a target="_blank" href="https://www.freepatentsonline.com/12304088.html">
             Systems and methods for calibrating deformable sensors (US12304088)
          </a></span></li> 

          <li><span><a target="_blank" href="https://www.freepatentsonline.com/12298198.html">
             Robots including a lift actuator and body structure for lifting objects (US12298198)
          </a></span></li> 

          <li><span><a target="_blank" href="https://www.freepatentsonline.com/12234137.html">
             Lifting robots (US12234137)
          </a></span></li> 


        </ul>
      </td>
    </tr> 


    <tr>
      <td class="table-title year">2024</td>
      <td class="table-links">

        <ul>

          <li><span><a target="_blank" href="https://www.freepatentsonline.com/12162738.html">
             Robots and sensor systems having compliant members (US12162738)
          </a></span></li> 

          <li><span><a target="_blank" href="https://www.freepatentsonline.com/12151361.html">
            Robust and perceptive manipulation through soft-bubble grippers and monocular depth (US12151361)
          </a></span></li> 

          <li><span><a target="_blank" href="https://www.freepatentsonline.com/12071335.html">
            Sensor devices including force sensors and robots incorporating the same (US12071335)
          </a></span></li> 

          <li><span><a target="_blank" href="https://www.freepatentsonline.com/12071334.html">
            Pressure sensor devices and robots including the same (US12071334)
          </a></span></li> 

          <li><span><a target="_blank" href="https://www.freepatentsonline.com/12065134.html">
            Wheel well mounted depth sensors for tire monitoring (US12065134)
          </a></span></li> 

          <li><span><a target="_blank" href="https://www.freepatentsonline.com/12048998.html">
            Systems and methods for estimating shape, contact forces, and pose of objects manipulated by robots having compliant contact and geometry sensors, cont. (US12048998)
          </a></span></li> 

          <li><span><a target="_blank" href="https://www.freepatentsonline.com/12017897.html">
            Robots having a lift actuator and a tilt structure for lifting and supporting large objects (US12017897)
          </a></span></li> 

          <li><span><a target="_blank" href="https://www.freepatentsonline.com/11993498.html">
            Structures and sensor assemblies having engagement structures for securing a compliant substrate assembly (US11993498)
          </a></span></li> 

          <li><span><a target="_blank" href="https://www.freepatentsonline.com/11951633.html">
            Systems for determining location using robots with deformable sensors, cont. (US11951633)
          </a></span></li> 

          <li><span><a target="_blank" href="https://www.freepatentsonline.com/11891288.html">
            Sensors having a deformable layer and a rugged cover layer and robots incorporating the same (US11891288)
          </a></span></li> 

        </ul>
      </td>
    </tr> 


    <tr>
      <td class="table-title year">2023</td>
      <td class="table-links">

        <ul>

          <li><span><a target="_blank" href="https://www.freepatentsonline.com/11819999.html">
            Input devices having a deformable membrane and methods of using the same (US11819999)
          </a></span></li> 

          <li><span><a target="_blank" href="https://www.freepatentsonline.com/11806864.html">
            Robot arm assemblies including fingers having deformable sensors, cont. (US11806864)
          </a></span></li>

          <li><span><a target="_blank" href="https://www.freepatentsonline.com/11667039.html">
            Systems and methods for estimating shape, contact forces, and pose of objects manipulated by robots having compliant contact and geometry sensors (US11667039)
          </a></span></li> 

          <li><span><a target="_blank" href="https://www.freepatentsonline.com/11628576.html">
            Deformable sensors and methods for detecting the pose and force on an object, cont. (US11628576)
          </a></span></li>

          <li><span><a target="_blank" href="https://www.freepatentsonline.com/11585715.html">
            Calibration device and calibration procedure for fluid filled sensor (US11585715)
          </a></span></li> 

          <li><span><a target="_blank" href="https://www.freepatentsonline.com/11584026.html">
            Robot arm assemblies including fingers having deformable sensors (US11584026)
          </a></span></li> 

          <li><span><a target="_blank" href="https://www.freepatentsonline.com/11577395.html">
            Systems for determining location using robots with deformable sensors (US11577395)
          </a></span></li> 

        </ul>
      </td>
    </tr> 


    <tr>
      <td class="table-title year">2022</td>
      <td class="table-links">

        <ul>

          <li><span><a target="_blank" href="https://www.freepatentsonline.com/11536621.html">
            Methods and systems for calibrating deformable sensors using camera (US11536621)
          </a></span></li> 

          <li><span><a target="_blank" href="https://www.freepatentsonline.com/11472039.html">
            Deformable sensor with rotatable sensing components for detecting deformation levels (US11472039)
          </a></span></li> 

          <li><span><a target="_blank" href="https://www.freepatentsonline.com/11465296.html">
            Deformable sensors and methods for detecting the pose and force on an object, cont. (US11465296)
          </a></span></li>

          <li><span><a target="_blank" href="https://www.freepatentsonline.com/11408788.html">
            Variable geometry and stiffness control for fluid filled sensor (US11408788)
          </a></span></li>                

          <li><span><a target="_blank" href="https://www.freepatentsonline.com/11389968.html">
            Systems and methods for determining pose of objects held by flexible end effectors (US11389968)
          </a></span></li>


        </ul>
      </td>
    </tr> 


    <tr>
      <td class="table-title year">2021</td>
      <td class="table-links">

        <ul>

          <li><span><a target="_blank" href="https://www.freepatentsonline.com/11185994.html">
            Deformable sensors having multiple time-of-flight emitters (US11185994)
          </a></span></li>


          <li><span><a target="_blank" href="https://www.freepatentsonline.com/11007652.html">
            Deformable sensors and methods for detecting the pose and force on an object, cont. (US11007652)
          </a></span></li>


        </ul>

      </td>
    </tr> 



    <tr>
      <td class="table-title year">2020</td>
      <td class="table-links">

        <ul>

          <li><span><a target="_blank" href="https://www.freepatentsonline.com/10668627.html">
            Deformable sensors and methods for detecting the pose and force on an object (US10668627)
          </a></span></li>

          <li><span><a target="_blank" href="https://www.freepatentsonline.com/10628968.html">
            Systems and methods of calibrating a depth-IR image offset (US10628968)
          </a></span></li>

          <li><span><a target="_blank" href="https://www.freepatentsonline.com/10549428.html">
            Robots with compliant contact and geometry sensors having varying touch sensitivity and methods for providing the same (US10549428)
          </a></span></li>

        </ul>

      </td>
    </tr> 




    <tr>
      <td class="table-title year">2019</td>
      <td class="table-links">

        <ul>

          <li><span><a target="_blank" href="https://www.freepatentsonline.com/10248085.html">
            Computational design of robots from high-level task specifications (US10248085)
          </a></span></li>

        </ul>

      </td>
    </tr> 

    <tr>
      <td class="table-title year">2017</td>
      <td class="table-links">

        <ul>

          <li><span><a target="_blank" href="https://www.freepatentsonline.com/9802314.html">
            Soft Body Robot for Physical Interaction with Humans (US9802314)
          </a></span></li>

        </ul>

      </td>
    </tr>      

  </table>
</div>  










<!--<h4>Controlling and Maximizing the Humanoid Robot Pushing Force by Postures</h4>-->




<!-- PAPERS ############ -->

<div id="hrdivider"> </div>




<h2>Publications<small> </small></h2>




<a name="LBM1paper" id="LBM1paper"></a>  
<div class="well">

  <div class="project-title">
    A Careful Examination of Large Behavior Models for<br>Multitask Dexterous Manipulation
  </div>
  TBA
  <br>
  The LBM Team at TRI
  <br>
  

  <div class="project-description">
    <p><strong>abstract: </strong>General-purpose robots promise a future where household assistance is ubiquitous and aging in place is supported by reliable, intelligent help. These robots will unlock human potential by enabling people to shape and interact with the physical world in transformative new ways. At the core of this transformation are Large Behavior Models (LBMs) — embodied AI systems that take in robot sensor data and output actions. LBMs are pretrained on large, diverse manipulation datasets and offer the key to realizing robust, general-purpose robotic intelligence.</p>

    <p>Yet despite their growing popularity, we still know surprisingly little about the nuances of what today’s LBMs actually offer. This uncertainty stems from the difficulty of conducting rigorous, large-scale evaluations in real-world robotics. As a result, progress in algorithm and dataset design is often guided by intuition rather than evidence, hampering progress. Our work aims to change that.</p>
  </div>

  <div id="link-table">
    <table>
      <tr>
        <td class="table-title"><span class="label label-info pull-right"> paper </span></td>
        <td><a target="_blank" href="https://arxiv.org/abs/2507.05331">A Careful Examination of Large Behavior Models for Multitask Dexterous Manipulation</a></td>
      </tr> 
    </table>
  </div>    

</div> 



<a name="PAPRLE" id="PAPRLE"></a>  
<div class="well">

  <div class="project-title">
    PAPRLE (Plug-And-Play Robotic Limb Environment): A Modular Ecosystem for Robotic Limbs
  </div>
  Obin Kwon, Sankalp Yamsani, Noboru Myers, Sean Taylor, Jooyoung Hong, Kyungseo Park, <strong>Alex Alspach</strong>, Joohyung Kim
  <br>
  

  <div class="project-description">
    <p><strong>abstract: </strong>We introduce PAPRLE (Plug-And-Play Robotic Limb Environment), a modular ecosystem that enables flexible placement and control of robotic limbs. With PAPRLE, a user can change the arrangement of the robotic limbs, and control them using a variety of input devices, including puppeteers, gaming controllers, and VR-based interfaces. This versatility supports a wide range of teleoperation scenarios and promotes adaptability to different task requirements.</p>

    <p>To further enhance configurability, we introduce a pluggable puppeteer device that can be easily mounted and adapted to match the target robot configurations. PAPRLE supports bilateral teleoperation through these puppeteer devices, agnostic to the type or configuration of the follower robot. By supporting both joint-space and task-space control, the system provides real-time force feedback, improving user fidelity and physical interaction awareness. The modular design of PAPRLE facilitates novel spatial arrangements of the limbs and enables scalable data collection, thereby advancing research in embodied AI and learning-based control. We validate PAPRLE in various real-world settings, demonstrating its versatility across diverse combinations of leader devices and follower robots.</p>
  </div>

  <div id="link-table">
    <table>
      <tr>
        <td class="table-title"><span class="label label-info pull-right"> project </span></td>
        <td><a target="_blank" href="https://uiuckimlab.github.io/paprle-pages/">PAPRLE Project Page</a></td>
      </tr> 
      <tr>
        <td class="table-title"><span class="label label-info pull-right"> video </span></td>
        <td><a target="_blank" href="https://youtu.be/7-N_Ps85GaU">PAPRLE Overview Video</a></td>
      </tr> 
    </table>
  </div>    

</div> 


<a name="OANeeds" id="OANeeds"></a>  
<div class="well">

  <div class="project-title">
    Uncovering Older Adult Needs – Applying User-Centered Research Methodologies to Inform Robotics Development and a Call to Action
  </div>
  <strong>Human-Robot Interaction - A Multidisciplinary Overview</strong>
  <br>
  Katherine M. Tsui, Sarah Cohen, Selma Šabanović, <strong>Alex Alspach</strong>, Rune Baggett, David Crandall and Steffi Paepcke
  <br>
  IntechOpen
  

  <div class="project-description">
    <p><strong>abstract: </strong>Aging society is a worldwide crisis that began in Japan (JP) and was followed by many more countries, including the United States (US). With this increase in the Older Adult (OA) population, it is pertinent to understand what OAs want for themselves and need to independently live in their own homes for as long as possible. This chapter catalogs our research between 2016 and 2023 about the needs of OAs in and around their homes. Using 10 user-centered research methodologies, we took a cross-cultural approach to conducting 69 studies in the US and JP. The primary goal of these studies was to identify the challenges OAs face in their daily lives and better understand user preferences for robotic assistance for such challenges. This grounded understanding is necessary to design both the robot and the interactions between the human and the robot. Our findings indicate five overarching themes about OA challenges: mobility and stability; moving heavy objects; dexterity; cognitive aging and social support; and sensory and physical decline. This chapter should be used as a guide to inspire the development of robotic technologies that OAs need and want to use and enable them to live independently longer.</p>
  </div>

  <div id="link-table">
    <table>
      <tr>
        <td class="table-title"><span class="label label-info pull-right"> chapter </span></td>
        <td><a target="_blank" href="https://www.intechopen.com/online-first/1201933">Uncovering Older Adult Needs – Applying User-Centered Research Methodologies to Inform Robotics Development and a Call to Action</a></td>
      </tr> 
    </table>
  </div>  

</div> 



<a name="EGRL" id="EGRL"></a>  
<div class="well">

  <div class="project-title">
    Learning Contact-Rich Whole-Body Manipulation with Example-Guided Reinforcement Learning
  </div>
  <strong>Science Robotics</strong>
  <br>
  Jose Barreiros, Aykut Onol, Mengchao Zhang, Sam Creasey, Aimee Goncalves,<br>Andrew Beaulieu, Aditya Bhat, Kate Tsui, <strong>Alex Alspach</strong>
  

  <div class="project-description">
    <strong>abstract: </strong>Humans employ a diversity of skills and strategies to effectively manipulate various objects, ranging from dexterous in-hand manipulation (fine motor skills) to complex whole-body manipulation (gross motor skills). The latter involves full-body engagement and extensive contact with various body parts beyond just the hands, where the compliance of our skin and muscles play a crucial role in increasing contact stability and mitigating uncertainty. For robots, synthesizing such contact-rich behaviors has fundamental challenges due to the rapidly growing combinatorics inherent to this amount of contact, making explicit reasoning about all contact interactions intractable. We explore the use of example-guided reinforcement learning to generate robust whole-body skills for the manipulation of large and unwieldy objects. Our method’s effectiveness is demonstrated on Toyota Research Institute’s Punyo robot, a humanoid upper-body with highly deformable, pressure-sensing skin. Training is conducted in simulation with only a single example motion per object manipulation task, and policies are easily transferred to hardware owing to domain randomization and the robot’s compliance. The resulting agent can manipulate various everyday objects, such as a water jug and large boxes, in a similar fashion to the example motion. Additionally, we show blind dexterous whole-body manipulation, relying solely on proprioceptive and tactile feedback without object pose tracking. Our analysis highlights the critical role of compliance in facilitating whole-body manipulation with humanoid robots. 
  </div>

  <div id="link-table">
    <table>
      <tr>
        <td class="table-title"><span class="label label-info pull-right"> article </span></td>
        <td><a target="_blank" href="https://www.science.org/doi/10.1126/scirobotics.ads6790">Learning Contact-Rich Whole-Body Manipulation with Example-Guided Reinforcement Learning</a></td>
      </tr> 
    </table>
  </div>    


</div> 



<a name="Grip" id="Grip"></a>  
<div class="well">

  <div class="project-title">
    Don't Lose Your Grip: Enhanced Grasp and Contact Estimation with a Passive Particle Jamming Visuotactile End Effector
  </div>
  Jooyoung Hong, Joohee Yim, <strong>Alex Alspach</strong>, Joohyung Kim
  <br>
  TBA
  

  
  <div class="project-description">
    <strong>abstract: </strong>Utilizing tactile sensors made of compliant materials for object manipulation provides the advantage of adaptive contact. Recent research has explored various methods for manipulating objects using soft sensors, but these approaches often face challenges in gripping objects securely because of their inherent flexibility. To address this problem, this paper presents a novel visuotactile sensing end effector based on the passive particle jamming effect (PPJE) capable of tactile sensing and stable grasping simultaneously. The proposed end effector consists of a soft membrane tip filled with particles and liquid with matched refractive indices for visibility, a camera, and a diaphragm chamber module. The soft tip conforms to contacting surfaces passively while the camera estimates contact position and force. When pressed, the liquid flows from the tip to the diaphragm chamber, causing the internal particles to condense and jam, surrounding and securing the contacting object. The end effector’s hardware development process, including design considerations, material selection, and fabrication, was studied. The end effector is calibrated using a neural network and its sensing characteristics are validated through indentation tests. The stable manipulation of the PPJE is validated through several grasping and sliding experiments.
  </div>

</div> 




<a name="Visuomotor" id="Visuomotor"></a>  
<div class="well">

  <div class="project-title">
    Exploring Locally Linear Representations for Visuomotor and Tactile Control
  </div>
  Paarth Shah, Naveen Kuppuswamy, Aykut Onol, Jose Barreiros, Andrew Beaulieu, Siyuan Feng, <strong>Alex Alspach</strong>, Russ Tedrake
  <br>
  RA-L, ICRA 2024
  

<!--
  
  <strong>abstract: </strong>text<br><br><br>

    <div id="link-table">
    <table>

      <tr>
        <td class="table-title"><span class="label label-info pull-right"> paper </span></td>
        <td><a target="_blank" href="https://arxiv.org/abs/2111.01376">SEED: Series Elastic End Effectors in 6D for Visuotactile Tool Use</a></td>
      </tr> 

      <tr>
        <td class="table-title"><span class="label label-info pull-right"> video </span></td>
        <td><a target="_blank" href="https://www.youtube.com/watch?v=2-YuIfspDrk">Video</a></td>
      </tr> 

  </table>
</div>

-->

</div> 




<a name="SEED" id="SEED"></a>  
<div class="well">


  <div class="project-title">
    SEED: Series Elastic End Effectors in 6D for Visuotactile Tool Use
  </div>
  H.J. Terry Suh, Naveen Kuppuswamy, Tao Pang, Paul Mitiguy, <strong>A. Alspach</strong>, Russ Tedrake
  <br>
  IROS 2022
  

  
  <div class="project-description">
    <strong>abstract: </strong>We propose the framework of Series Elastic End Effectors in 6D (SEED), which combines a spatially compliant element with visuotactile sensing to grasp and manipulate tools in the wild. Our framework generalizes the benefits of series elasticity to 6-dof, while providing an abstraction of control using visuotactile sensing. We propose an algorithm for relative pose estimation from visuotactile sensing, and a spatial hybrid force-position controller capable of achieving stable force interaction with the environment. We demonstrate the effectiveness of our framework on tools that require regulation of spatial forces.
  </div>

  
  <div id="link-table">
    <table>

      <!--
      <tr>
        <td class="table-title"><span class="label label-info pull-right"> arxiv </span></td>
        <td><a target="_blank" href="https://arxiv.org/abs/1904.02252">Publication Page</a></td>
      </tr>
    -->

      <tr>
        <td class="table-title"><span class="label label-info pull-right"> paper </span></td>
        <td><a target="_blank" href="https://arxiv.org/abs/2111.01376">SEED: Series Elastic End Effectors in 6D for Visuotactile Tool Use</a></td>
      </tr> 


      <tr>
        <td class="table-title"><span class="label label-info pull-right"> video </span></td>
        <td><a target="_blank" href="https://www.youtube.com/watch?v=2-YuIfspDrk">Video</a></td>
      </tr> 

    </table>
  </div>
</div> 






<a name="Punyo1" id="Punyo1"></a>  
<div class="well">




  <div class="project-title">
    Punyo-1: Soft Tactile-Sensing Upper-Body Robot for Large Object Manipulation and Physical Human Interaction
  </div>
  A. Goncalves, N. Kuppuswamy, A. Beaulieu, A. Uttamchandani, K.M. Tsui, <strong>A. Alspach</strong>
  <br>
  RoboSoft 2022
  
  


  <div class="project-description">
    <strong>abstract: </strong>The manipulation of large objects and safe operation in the vicinity of humans are key capabilities of a general purpose domestic robotic assistant. We present the design of a soft, tactile-sensing humanoid upper-body robot and demonstrate whole-body rich-contact manipulation strategies for handling large objects. We demonstrate our hardware design philosophy for outfitting off-the-shelf hard robot arms and other components with soft tactile-sensing modules, including: (i) low-cost, cut-resistant, contact pressure localizing coverings for the arms, (ii) paws based on TRI's Soft-bubble sensors for the end effectors, and (iii) compliant force/geometry sensors for the coarse geometry sensing chest. We leverage the mechanical intelligence and tactile sensing of these modules to develop and demonstrate motion primitives for whole-body grasping. We evaluate the hardware's effectiveness in achieving grasps of varying strengths over a variety of large domestic objects. Our results demonstrate the importance of exploiting softness and tactile sensing in contact-rich manipulation strategies, as well as a path forward for whole-body force-controlled interactions with the world.
  </div>

  
  <div id="link-table">
    <table>

      <!--
      <tr>
        <td class="table-title"><span class="label label-info pull-right"> arxiv </span></td>
        <td><a target="_blank" href="https://arxiv.org/abs/1904.02252">Publication Page</a></td>
      </tr>
    -->

    <tr>
      <td class="table-title"><span class="label label-info pull-right"> paper </span></td>
      <td><a target="_blank" href="https://arxiv.org/abs/2111.09354">Punyo-1: Soft Tactile-Sensing Upper-Body Robot for Large Object Manipulation and Physical Human Interaction</a></td>
    </tr> 


    <tr>
      <td class="table-title"><span class="label label-info pull-right"> video </span></td>
      <td><a target="_blank" href="https://www.youtube.com/watch?v=WNXSAK6pF0M">Video</a></td>
    </tr> 

  </table>
</div>
</div> 






<a name="ActivePressure1" id="ActivePressure1"></a>  
<div class="well">

  <div class="project-title">
    Variable Compliance and Geometry Regulation of Soft-Bubble Grippers with Active Pressure Control
  </div>
  S. Joonhigh, N. Kuppuswamy, A. Beaulieu, <strong>A. Alspach</strong>, R. Tedrake
  <br>
  RoboSoft 2021
  
  

  <div class="project-description">
    <strong>abstract: </strong>While compliant grippers have become increasingly commonplace in robot manipulation, finding the right stiffness and geometry for grasping the widest variety of objects remains a key challenge. Adjusting both stiffness and gripper geometry on the fly may provide the versatility needed to manipulate the large range of objects found in domestic environments. We present a system for actively controlling the geometry (inflation level) and compliance of Soft-bubble grippers - air filled, highly compliant parallel gripper fingers incorporating visuotactile sensing. The proposed system enables large, controlled changes in gripper finger geometry and grasp stiffness, as well as simple in-hand manipulation. We also demonstrate, despite these changes, the continued viability of advanced perception capabilities such as dense geometry and shear force measurement - we present a straightforward extension of our previously presented approach for measuring shear induced displacements using the internal imaging sensor and taking into account pressure and geometry changes. We quantify the controlled variation of grasp-free geometry, grasp stiffness and contact patch geometry resulting from pressure regulation and we demonstrate new capabilities for the gripper in the home by grasping in constrained spaces, manipulating tools requiring lower and higher stiffness grasps, as well as contact patch modulation.  
  </div>  

  <div id="link-table">
    <table>

      <!--
      <tr>
        <td class="table-title"><span class="label label-info pull-right"> arxiv </span></td>
        <td><a target="_blank" href="https://arxiv.org/abs/1904.02252">Publication Page</a></td>
      </tr>
    -->

    <tr>
      <td class="table-title"><span class="label label-info pull-right"> paper </span></td>
      <td><a target="_blank" href="https://arxiv.org/abs/2103.08710">Variable Compliance and Geometry Regulation of Soft-Bubble Grippers with Active Pressure Control</a></td>
    </tr> 


    <tr>
      <td class="table-title"><span class="label label-info pull-right"> video </span></td>
      <td><a target="_blank" href="https://www.youtube.com/watch?v=1ywIaWuwTbI">Video</a></td>
    </tr> 

  </table>
</div>
</div> 






<a name="Monocular1" id="Monocular1"></a>  
<div class="well">

  <div class="project-title">
    Monocular Depth Estimation for Soft Visuotactile Sensors
  </div>
  R. Ambrus, V. Guizilini, N. Kuppuswamy, A. Beaulieu, A. Gaidon, <strong>A. Alspach</strong>
  <br>
  RoboSoft 2021
  
  


  <div class="project-description">
    <strong>abstract: </strong>Fluid-filled soft visuotactile sensors such as the Soft-bubbles alleviate key challenges for robust manipulation, as they enable reliable grasps along with the ability to obtain high-resolution sensory feedback on contact geometry and forces. Although they are simple in construction, their utility has been limited due to size constraints introduced by enclosed custom IR/depth imaging sensors to directly measure surface deformations. Towards mitigating this limitation, we investigate the application of state-of-the-art monocular depth estimation to infer dense internal (tactile) depth maps directly from the internal single small IR imaging sensor. Through real-world experiments, we show that deep networks typically used for long-range depth estimation (1-100m) can be effectively trained for precise predictions at a much shorter range (1-100mm) inside a mostly textureless deformable fluid-filled sensor. We propose a simple supervised learning process to train an object-agnostic network requiring less than 10 random poses in contact for less than 10 seconds for a small set of diverse objects (mug, wine glass, box, and fingers in our experiments). We show that our approach is sample-efficient, accurate, and generalizes across different objects and sensor configurations unseen at training time. Finally, we discuss the implications of our approach for the design of soft visuotactile sensors and grippers.    
  </div>


  <div id="link-table">
    <table>

      <!--
      <tr>
        <td class="table-title"><span class="label label-info pull-right"> arxiv </span></td>
        <td><a target="_blank" href="https://arxiv.org/abs/1904.02252">Publication Page</a></td>
      </tr>
    -->

    <tr>
      <td class="table-title"><span class="label label-info pull-right"> paper </span></td>
      <td><a target="_blank" href="https://arxiv.org/abs/2101.01677">Monocular Depth Estimation for Soft Visuotactile Sensors</a></td>
    </tr> 


    <tr>
      <td class="table-title"><span class="label label-info pull-right"> video </span></td>
      <td><a target="_blank" href="https://www.youtube.com/watch?v=MbWUxKK_jhw">Video</a></td>
    </tr> 

  </table>
</div>
</div> 









<a name="BubbleGripper1" id="BubbleGripper1"></a>  
<div class="well">

  <div class="project-title">
    Soft-Bubble grippers for robust and perceptive manipulation
  </div>
  N. Kuppuswamy, <strong>A. Alspach</strong>, A. Uttamchandani, S. Creasey, T. Ikeda, R. Tedrake
  <br>
  IROS 2020
  
  

  <div class="project-description">
    <strong>abstract: </strong>Manipulation in cluttered environments like homes requires stable grasps, precise placement and robustness against external contact. We present the Soft-Bubble gripper system with a highly compliant gripping surface and dense-geometry visuotactile sensing, capable of multiple kinds of tactile perception. We first present various mechanical design advances and a fabrication technique to deposit custom patterns to the internal surface of the sensor that enable tracking of shear-induced displacement of the manipuland. The depth maps output by the internal imaging sensor are used in an in-hand proximity pose estimation framework -- the method better captures distances to corners or edges on the manipuland geometry. We also extend our previous work on tactile classification and integrate the system within a robust manipulation pipeline for cluttered home environments. The capabilities of the proposed system are demonstrated through robust execution multiple real-world manipulation tasks.   
  </div> 

  <div id="link-table">
    <table>

      <!--
      <tr>
        <td class="table-title"><span class="label label-info pull-right"> arxiv </span></td>
        <td><a target="_blank" href="https://arxiv.org/abs/1904.02252">Publication Page</a></td>
      </tr>
    -->

    <tr>
      <td class="table-title"><span class="label label-info pull-right"> paper </span></td>
      <td><a target="_blank" href="https://arxiv.org/abs/2004.03691">Soft-Bubble grippers for robust and perceptive manipulation</a></td>
    </tr> 


    <tr>
      <td class="table-title"><span class="label label-info pull-right"> video </span></td>
      <td><a target="_blank" href="https://www.youtube.com/watch?v=zm-sdgb7KAA">Video</a></td>
    </tr> 

  </table>
</div>
</div> 






<a name="PatchContact" id="PatchContact"></a>  
<div class="well">
  <div class="project-title">
    Fast Model-Based Contact Patch and Pose Estimation for Highly Deformable Dense-Geometry Tactile Sensors
  </div>
  N. Kuppuswamy, A. Castro, C. Phillips-Grafflin, <strong>A. Alspach</strong>, R. Tedrake
  <br>
  IEEE Robotics and Automation Letters (RA-L), Apr. 2020
  <br>
  <i class="fa fa-trophy" aria-hidden="true"></i> Best Paper Award

  
  

  <div class="project-description">
    <strong>abstract: </strong>The difficulty of modelling deformable contact is a well-known problem in soft robotics and is particularly acute for compliant interfaces that permit large deformations, where the problem of inferring exact contact locations and manipuland pose is challenging. We present a model for the behavior of a highly-deformable dense geometry sensor in its interaction with objects; the forward model predicts the deformation of a mesh given the pose and geometry of a contacting rigid object. We use this model to develop a fast approximation to solve the inverse problem: estimating the contact patch when the sensor is deformed by arbitrary objects. This inverse model can be easily identified through experiments and is formulated as a sparse Quadratic Program (QP) that can be solved efficiently online. The proposed model serves as the first stage of a pose estimation pipeline for robot manipulation. We demonstrate the proposed inverse model through real-time estimation of contact patches on a contact-rich manipulation problem in which oversized fingers screw a nut onto a bolt, and as part of a complete pipeline for ICP-based pose-estimation and tracking. Our results demonstrate a path towards realizing soft robots with highly compliant surfaces that perform complex real-world manipulation tasks.  
  </div>  

  <div id="link-table">
    <table>

      <!--
      <tr>
        <td class="table-title"><span class="label label-info pull-right"> arxiv </span></td>
        <td><a target="_blank" href="https://arxiv.org/abs/1904.02252">Publication Page</a></td>
      </tr>
    -->

    <tr>
      <td class="table-title"><span class="label label-info pull-right"> paper </span></td>
      <td><a target="_blank" href="files/papers/2019-Kuppuswamy_Castro_Phillips-Grafflin_Alspach_Tedrake-Contact_Patch_notfinal.pdf">Fast Model-Based Contact Patch and Pose Estimation for Highly Deformable Dense-Geometry Tactile Sensors</a></td>
    </tr> 


    <tr>
      <td class="table-title"><span class="label label-info pull-right"> video </span></td>
      <td><a target="_blank" href="https://www.youtube.com/watch?v=pk-QBCJm7_w">Video</a></td>
    </tr> 




  </table>
</div>

</div> 



<a name="SmoothContact" id="SmoothContact"></a>  
<div class="well">

  <div class="project-title">
    A Transition-Aware Method for the Simulation of Compliant Contact with Regularized Friction
  </div>
  A. Castro, A. Qu, N. Kuppuswamy, <strong>A. Alspach</strong>, M. Sherman
  <br>
  IEEE Robotics and Automation Letters (RA-L), Apr. 2020
  

  <div class="project-description">
    <strong>abstract: </strong>Multibody simulation with frictional contact has been a challenging subject of research for the past thirty years. Rigid-body assumptions are commonly used to approximate the physics of contact, and together with Coulomb friction, lead to challenging-to-solve nonlinear complementarity problems (NCP). On the other hand, robot grippers often introduce significant compliance. Compliant contact, combined with reg- ularized friction, can be modeled entirely with ODEs, avoiding NCP solves. Unfortunately, regularized friction introduces high- frequency stiff dynamics and even implicit methods struggle with these systems, especially during slip-stick transitions. To improve the performance of implicit integration for these systems we introduce a Transition-Aware Line Search (TALS), which greatly improves the convergence of the Newton-Raphson iterations performed by implicit integrators. We find that TALS works best with semi-implicit integration, but that the explicit treatment of normal compliance can be problematic. To address this, we develop a Transition-Aware Modified Semi-Implicit (TAMSI) integrator that has similar computational cost to semi- implicit methods but implicitly couples compliant contact forces, leading to a more robust method. We evaluate the robustness, accuracy and performance of TAMSI and demonstrate our approach alongside a sim-to-real manipulation task.
  </div>
     


  <div id="link-table">
    <table>

      <!--
      <tr>
        <td class="table-title"><span class="label label-info pull-right"> arxiv </span></td>
        <td><a target="_blank" href="https://arxiv.org/abs/1904.02252">Publication Page</a></td>
      </tr>
    -->

    <tr>
      <td class="table-title"><span class="label label-info pull-right"> paper </span></td>
      <td><a target="_blank" href="files/papers/2019-Castro_Qu_Kuppuswamy_Alspach_Sheman-Smooth_Contact_notfinal.pdf">A Transition-Aware Method for the Simulation of Compliant Contact with Regularized Friction</a></td>
    </tr> 


    <tr>
      <td class="table-title"><span class="label label-info pull-right"> video </span></td>
      <td><a target="_blank" href="https://youtu.be/d8dXsxtGwpE">Video</a></td>
    </tr> 




  </table>
</div>

</div> 



<a name="SoftBubble" id="SoftBubble"></a>  
<div class="well">

  <div class="project-title">
    Soft-bubble: A highly compliant dense geometry tactile sensor for robot manipulation
  </div>
  <strong>A. Alspach</strong>, K. Hashimoto, N. Kuppuswamy, R. Tedrake
  <br>
  IEEE-RAS International Conference on Soft Robotics (RoboSoft), Seoul, Korea, Apr. 2019.
  

  <div class="project-description">
    <strong>abstract: </strong>Incorporating effective tactile sensing and mechanical compliance is key towards enabling robust and safe operation of robots in unknown, uncertain and cluttered environments. Towards realizing this goal, we present a lightweight, easy-to-build, highly compliant dense geometry sensor and end effector that comprises an inflated latex membrane with a depth sensor behind it. We present the motivations and the hardware design for this Soft-bubble and demonstrate its capabilities through example tasks including tactile-object classification, pose estimation and tracking, and nonprehensile object manipulation. We also present initial experiments to show the importance of high-resolution geometry sensing for tactile tasks and discuss applications in robust manipulation.   
  </div>  

  <div id="link-table">
    <table>

      <tr>
        <td class="table-title"><span class="label label-info pull-right"> arxiv </span></td>
        <td><a target="_blank" href="https://arxiv.org/abs/1904.02252">Publication Page</a></td>
      </tr>

      <tr>
        <td class="table-title"><span class="label label-info pull-right"> paper </span></td>
        <td><a target="_blank" href="files/papers/2019-Alspach_Hashimoto_Kuppuswamy_Tedrake-Soft-bubble.pdf">Soft-bubble: A highly compliant dense geometry tactile sensor for robot manipulation (RoboSoft 2019)</a></td>
      </tr> 

      <tr>
        <td class="table-title"><span class="label label-info pull-right"> video </span></td>
        <td><a target="_blank" href="https://www.youtube.com/watch?v=sDfNkJzZ7RY">Published Video</a></td>
      </tr> 

      

    </table>
  </div>

</div> 






<a name="CompDes2" id="CompDes2"></a>  
<div class="well">

  <div class="project-title">
    Computational Design of Robotic Devices from High-Level Motion Specification
  </div>
  S. Ha, S. Coros, <strong>A. Alspach</strong>, J. Bern, J. Kim, K. Yamane
  <br>
  IEEE Transactions on Robotics (T-RO), Jul. 2018.
  

  <div id="link-table">
    <table>

      <tr>
        <td class="table-title"><span class="label label-info pull-right"> page </span></td>
        <td><a target="_blank" href="https://la.disneyresearch.com/publication/computational-design-of-robotic-devices-from-high-level-motion-specifications/">Disney Research Publication Page</a></td>
      </tr>

      <tr>
        <td class="table-title"><span class="label label-info pull-right"> paper </span></td>
        <td><a target="_blank" href="files/papers/2018-Ha_Coros_Alspach_Bern_Kim_Yamane-Computational_Design_of_Robotic_Devices_from_High_Level_Motion_Specifications.pdf">Computational Design of Robotic Devices from High-Level Motion Specifications (T-RO 2018)</a></td>
      </tr> 

      <tr>
        <td class="table-title"><span class="label label-info pull-right"> video </span></td>
        <td><a target="_blank" href="https://www.youtube.com/watch?v=tyzUR_vilDw">Published Video</a></td>
      </tr> 

      

    </table>
  </div>

</div> 



<a name="CompDes1" id="CompDes1"></a>  
<div class="well">

  <div class="project-title">
    Computational Co-Optimization of Design Parameters and Motion Trajectories<br>for Robotic Systems
  </div>
  S. Ha, S. Coros, <strong>A. Alspach</strong>, J. Kim, K. Yamane
  <br>
  International Journal of Robotics Research (IJRR), Jun. 2018.
  

  <div id="link-table">
    <table>

      <tr>
        <td class="table-title"><span class="label label-info pull-right"> page </span></td>
        <td><a target="_blank" href="https://la.disneyresearch.com/publication/computational-co-optimization-of-design-parameters-and-motion-trajectories-for-robotic-systems/">Disney Research Publication Page</a></td>
      </tr>

      <tr>
        <td class="table-title"><span class="label label-info pull-right"> paper </span></td>
        <td><a target="_blank" href="files/papers/2018-Ha_Coros_Alspach_Kim_Yamane-Computational_Co-Optimization_of_Design_Parameters_and_Motion_Trajectories_for_Robotic_Systems.pdf">Computational Co-Optimization of Design Parameters and Motion Trajectories for Robotic Systems (IJRR 2018)</a></td>
      </tr> 

      <tr>
        <td class="table-title"><span class="label label-info pull-right"> video </span></td>
        <td><a target="_blank" href="https://www.youtube.com/watch?v=iBcI4RVI-aY">Published Video</a></td>
      </tr> 

      

    </table>
  </div>

</div> 




<a name="SoftArm" id="SoftArm"></a>  
<div class="well">

  <div class="project-title">
    Design and Fabrication of a Soft Robotic Hand and Arm System
  </div>
  <strong>A. Alspach</strong>, J. Kim, K. Yamane
  <br>
  IEEE-RAS International Conference on Soft Robotics (RoboSoft), Livorno, Italy, Apr. 2018.
  

  <div id="link-table">
    <table>

      <tr>
        <td class="table-title"><span class="label label-info pull-right"> page </span></td>
        <td><a target="_blank" href="https://la.disneyresearch.com/publication/soft-robotic-hand-and-arm-system/">Disney Research Publication Page</a></td>
      </tr>

      <tr>
        <td class="table-title"><span class="label label-info pull-right"> paper </span></td>
        <td><a target="_blank" href="files/papers/2018-Alspach_Kim_Yamane-Design_and_Fabrication_of_a_Soft_Robotic_Hand_and_Arm_System.pdf">Design and Fabrication of a Soft Robotic Hand and Arm System (RoboSoft 2018)</a></td>
      </tr> 

      <tr>
        <td class="table-title"><span class="label label-info pull-right"> video </span></td>
        <td><a target="_blank" href="https://www.youtube.com/watch?v=pLPDHm4E9Ew">RoboSoft Submission Video</a></td>
      </tr> 

      <tr>
        <td class="table-title"><span class="label label-info pull-right"> poster </span></td>
        <td><a target="_blank" href="files/presentations/2018-Alspach_Kim_Yamane-Design_and_Fabrication_of_a_Soft_Robotic_Hand_and_Arm_System_poster.pdf">RoboSoft Poster</a></td>
      </tr>      

      

    </table>
  </div>

</div> 


<!-- 2017 -->

<a name="Snapbot1" id="Snapbot1"></a>  
<div class="well">

  <div class="project-title">
    Snapbot: A Reconfigurable Legged Robot
  </div>
  J. Kim, <strong>A. Alspach</strong>, K. Yamane
  <br>
  International Conference on Intelligent Robots and Systems (IROS), Vancouver, Canada, Sept. 2017.
  

  <div id="link-table">
    <table>

      <tr>
        <td class="table-title"><span class="label label-info pull-right"> page </span></td>
        <td><a target="_blank" href="https://la.disneyresearch.com/publication/snapbot/">Disney Research Publication Page</a></td>
      </tr>

      <tr>
        <td class="table-title"><span class="label label-info pull-right"> paper </span></td>
        <td><a target="_blank" href="files/papers/2017-Kim_Alspach_Yamane-Snapbot_a_Reconfigurable_Legged_Robot.pdf">Snapbot: A Reconfigurable Legged Robot (IROS 2017)</a></td>
      </tr> 

      <tr>
        <td class="table-title"><span class="label label-info pull-right"> video </span></td>
        <td><a target="_blank" href="https://www.youtube.com/watch?v=PpdFCd2KQ8o">IROS Submission Video</a></td>
      </tr> 

      

    </table>
  </div>

</div> 





<a name="ImpFuncTheor" id="ImpFuncTheor"></a>  
<div class="well">

  <div class="project-title">
    Joint Optimization of Robot Design and Motion Parameters using<br>the Implicit Function Theorem
  </div>
  S. Ha, S. Coros, <strong>A. Alspach</strong>, J. Kim, K. Yamane
  <br>
  Robotics Science and Systems (RSS), MIT, Cambridge, MA, July 2017.
  <br>
  <i class="fa fa-trophy" aria-hidden="true"></i> Best Paper Award Finalist
  

  <div id="link-table">
    <table>

      <tr>
        <td class="table-title"><span class="label label-info pull-right"> page </span></td>
        <td><a target="_blank" href="https://la.disneyresearch.com/publication/joint-optimization-of-robot-design-and-motion-parameters-using-the-implicit-function-theorem/">Disney Research Publication Page</a></td>
      </tr>

      <tr>
        <td class="table-title"><span class="label label-info pull-right"> paper </span></td>
        <td><a target="_blank" href="files/papers/2017-Ha_Coros_Alspach_Kim_Yamane-Joint_Optimization_of_Robot_Design_and_Motion_Parameters_using_the_Implicit_Function_Theorem.pdf">Joint Optimization of Robot Design and Motion Parameters using the Implicit Function Theorem (RSS 2017)</a></td>
      </tr> 

      <tr>
        <td class="table-title"><span class="label label-info pull-right"> pic </span></td>
        <td><a target="_blank" href="../photos/projects/DisneyDesignOptimization/2017-Alspach_Ha_Kim-RSS-Finalist.jpg">Best Paper Award Finalists</a></td>
      </tr> 


      <tr>
        <td class="table-title"><span class="label label-info pull-right"> video </span></td>
        <td><a target="_blank" href="https://www.youtube.com/watch?v=jr4bg09ZnYM">RSS Submission Video</a></td>
      </tr> 

      

    </table>
  </div>

</div> 





<a name="LimbOptPaper" id="LimbOptPaper"></a>  
<div class="well">

  <div class="project-title">
    Task-Based Limb Optimization for Legged Robots
  </div>
  S. Ha, S. Coros, <strong>A. Alspach</strong>, J. Kim, K. Yamane
  <br>
  IEEE/RSJ Int'l Conf. on Intelligent Robots and Systems (IROS), Daejeon, South Korea, Oct. 2016.
  

  <div id="link-table">
    <table>

      <tr>
        <td class="table-title"><span class="label label-info pull-right"> page </span></td>
        <td><a target="_blank" href="https://la.disneyresearch.com/publication/task-based-limb-optimization-for-legged-robots/">Disney Research Publication Page</a></td>
      </tr>

      <tr>
        <td class="table-title"><span class="label label-info pull-right"> paper </span></td>
        <td><a target="_blank" href="files/papers/2016-Ha_Coros_Alspach_Kim_Yamane-Task_based_Limb_Optimization_for_Legged_Robots.pdf">Task-based Limb Optimization for Legged Robots (IROS 2016)</a></td>
      </tr> 

    </table>
  </div>

</div> 


<a name="SoftUkelelePaper" id="SoftUkelelePaper"></a>  
<div class="well">

  <div class="project-title">
    Mechanical Implementation of a Variable-Stiffness Actuator for a Softly Strummed Ukulele
  </div>
  A. Lawrence, <strong>A. Alspach</strong>, D. Bentivegna
  <br>
  IEEE/RSJ Int'l Conf. on Intelligent Robots and Systems (IROS), Daejeon, South Korea, Oct. 2016.
  

  <div id="link-table">
    <table>

      <tr>
        <td class="table-title"><span class="label label-info pull-right"> page </span></td>
        <td><a target="_blank" href="https://la.disneyresearch.com/publication/mechanical-implementation-of-a-variable-stiffness-actuator-for-a-softly-strummed-ukulele/">Disney Research Publication Page</a></td>
      </tr>

      <tr>
        <td class="table-title"><span class="label label-info pull-right"> paper </span></td>
        <td><a target="_blank" href="files/papers/2016-Lawrence_Alspach_Bentivegna-Mechanical_Implementation_of_a_Variable_Stiffness_Actuator_for_a_Softly_Strummed_Ukulele.pdf">Mechanical Implementation of a Variable-Stiffness Actuator for a Softly Strummed Ukulele (IROS 2016)</a></td>
      </tr> 

    </table>
  </div>


</div> 


<a name="HuggingPaper" id="HuggingPaper"></a>  
<div class="well">

  <div class="project-title">
    Study of Children's Hugging for Interactive Robot Design
  </div>
  J. Kim, <strong>A. Alspach</strong>, I. Leite, K. Yamane
  <br>
  IEEE Int'l Symposium on Robot and Human Interactive Communication, NYC, Aug. 2016.
  

  <div id="link-table">
    <table>

      <tr>
        <td class="table-title"><span class="label label-info pull-right"> page </span></td>
        <td><a target="_blank" href="https://la.disneyresearch.com/publication/study-of-childrens-hugging-for-interactive-robot-design/">Disney Research Publication Page</a></td>
      </tr>

      <tr>
        <td class="table-title"><span class="label label-info pull-right"> paper </span></td>
        <td><a target="_blank" href="files/papers/2016-Kim_Alspach_Leite_Yamane-Study_of_Childrens_Hugging_for_Interactive_Robot_Design.pdf">Study of Children's Hugging for Interactive Robot Design (RO-MAN 2016)</a></td>
      </tr> 

    </table>
  </div>



</div> 



<a name="TeleHeadPaper" id="TeleHeadPaper"></a>  
<div class="well">

  <div class="project-title">
    Imitating Human Movement with Teleoperated Robotic Head
  </div>
  P. Agarwal, S. Al Moubayed, <strong>A. Alspach</strong>, J. Kim, E. Carter, J. Lehman, K. Yamane
  <br>
  IEEE Int'l Symposium on Robot and Human Interactive Communication, NYC, Aug. 2016.
  <br>
  <i class="fa fa-trophy" aria-hidden="true"></i> Best Technical Paper Award
  

  <div id="link-table">
    <table>


      <tr>
        <td class="table-title"><span class="label label-info pull-right"> page </span></td>
        <td><a target="_blank" href="https://la.disneyresearch.com/publication/imitating-human-movement-with-teleoperated-robotic-head/">Disney Research Publication Page</a></td>
      </tr>


      <tr>
        <td class="table-title"><span class="label label-info pull-right"> paper </span></td>
        <td><a target="_blank" href="files/papers/2016-Agarwal_AlMoubayed_Alspach_Kim_Carter_Lehman_Yamane-Imitating_Human_Movement_with_Teleoperated_Robotic_Head.pdf">Imitating Human Movement with Teleoperated Robotic Head (RO-MAN 2016)</a></td>
      </tr> 


      <tr>
        <td class="table-title"><span class="label label-info pull-right"> pic </span></td>
        <td><a target="_blank" href="../photos/projects/DisneyTeleopHead/2016-Agarwal_Alspach-RO-MAN_Best_Paper_Certificate.jpg">Best Technical Paper Award Certificate</a></td>
      </tr> 


    </table>
  </div>



</div>  







<a name="SoftUpperPaper" id="SoftUpperPaper"></a>  
<div class="well">

  <div class="project-title">
    Design of a Soft Upper Body Robot for Physical Human-Robot Interaction
  </div>
  <strong>A. Alspach</strong>, J. Kim, K. Yamane
  <br>
  Proc. IEEE-RAS Int'l Conf. Humanoid Robotics, Seoul, South Korea, Nov. 2015.
  <br>
  <i class="fa fa-trophy" aria-hidden="true"></i> Best Oral Paper Award Finalist
  
  <div id="link-table">
    <table>

      <tr>
        <td><span class="label label-info pull-right"> page </span></td>
        <td><a target="_blank" href="https://la.disneyresearch.com/publication/design-of-a-soft-upper-body-robot/">Disney Research Publication Page</a></td>
      </tr>

      <tr>
        <td><span class="label label-info pull-right"> paper </span></td>
        <td><a target="_blank" href="files/papers/2015-Alspach_Kim_Yamane-Humanoids-Design_of_a_Soft_Upper_Body_Robot_for_Physical_Human_Robot_Interaction.pdf">Design of a Soft Upper Body Robot for Physical Human-Robot Interaction (Humanoids 2015)</a></td>
      </tr> 
      <tr>
        <td class="table-title"><span class="label label-info pull-right"> pic </span></td>
        <td><a target="_blank" href="../photos/projects/DisneySoftHumanoid/2015-Alspach_Kim-Humanoids-Finalist.jpg">Best Oral Paper Award Finalists</a></td>
      </tr> 
      <tr>
        <td class="table-title"><span class="label label-info pull-right"> pic </span></td>
        <td><a target="_blank" href="../photos/projects/DisneySoftHumanoid/2015-Alspach_Kim-Humanoids-Finalist-Certificate.jpg">Best Oral Paper Award Finalist Certificate</a></td>
      </tr> 

      <!-- 
      <tr>
        <td><span class="label label-info pull-right"> paper </span></td>
        <td><a target="_blank" href="http://la.disneyresearch.com/wp-content/uploads/3D-Printed-Soft-Skin-for-Safe-Human-Robot-Interaction-Paper.pdf">3D Printed Soft Skin for Safe Human-Robot Interaction (IROS 2015)</a></td>
      </tr> 
    -->           
  </table>
</div>  
</div>  


<a name="SkinShape" id="SkinShape"></a>  
<div class="well">

  <div class="project-title">
    Analyzing Muscle Activity and Force with Skin Shape Captured by Non-contact Visual Sensor
  </div>
  R. Sagawa, Y. Yoshiyasu, <strong>A. Alspach</strong>, K. Ayusawa, K. Yamane, A. Hilton
  <br>
  Proc. 7th Pacific Rim Symposium on Image and Video Technology (PSIVT2015), Auckland, New Zealand, Nov. 2015.

  
  
  <div id="link-table">
    <table>
      <tr>
        <td class="table-title"><span class="label label-info pull-right"> paper </span></td>
        <td><a target="_blank" href="files/papers/2016-Sagawa_Alspach-Analyzing_Muscle_Activity_and_Force_with_Skin_Shape_Captured_by_Non-contact_Visual_Sensor.pdf">Analyzing Muscle Activity and Force with Skin Shape Captured by Non-contact Visual Sensor</a></td>
      </tr>          
    </table>
  </div>  

</div>  




<!--
   _____        __ _      _____ _    _         __  __           _       _      
  / ____|      / _| |    / ____| |  (_)       |  \/  |         | |     | |     
 | (___   ___ | |_| |_  | (___ | | ___ _ __   | \  / | ___   __| |_   _| | ___ 
  \___ \ / _ \|  _| __|  \___ \| |/ / | '_ \  | |\/| |/ _ \ / _` | | | | |/ _ \
  ____) | (_) | | | |_   ____) |   <| | | | | | |  | | (_) | (_| | |_| | |  __/
 |_____/ \___/|_|  \__| |_____/|_|\_\_|_| |_| |_|  |_|\___/ \__,_|\__,_|_|\___|
      
http://patorjk.com/software/taag/#p=display&f=Big&t=Hubo%20Wiki FONT: Big                                                                               
-->

<a name="SoftSkinPaper" id="SoftSkinPaper"></a>  
<div class="well">

  <div class="project-title">
    3D Printed Soft Skin for Safe Human-Robot Interaction
  </div>
  J. Kim, <strong>A. Alspach</strong>, K. Yamane
  <br>
  Proc. IEEE/RSJ Int'l Conf. on Intelligent Robots and Systems (IROS), Hamburg, Germany, Sept. 2015.
  

  <div id="link-table">
    <table>
      <tr>
        <td class="table-title"><span class="label label-info pull-right"> page </span></td>
        <td><a target="_blank" href="http://la.disneyresearch.com/publication/3d-printed-soft-skin/">Disney Research Publication Page</a></td>
      </tr>  
      <tr>
        <td><span class="label label-info pull-right"> paper </span></td>
        <td><a target="_blank" href="files/papers/2015-Kim_Alspach_Yamane-3D_Printed_Soft_Skin_for_Safe_Human-Robot_Interaction.pdf">3D Printed Soft Skin for Safe Human-Robot Interaction (IROS 2015)</a></td>
      </tr>      
      <tr>
        <td><span class="label label-info pull-right"> video </span></td>
        <td><a target="_blank" href="https://www.youtube.com/watch?v=gdoQvQzPiZY">3D Printed Soft Skin for Safe Human-Robot Interaction Video (IROS 2015)</a></td>
      </tr>     
    </table>
  </div>  
</div>  











<a name="HumanoidRobotPushing" id="HumanoidRobotPushing"></a>  
<div class="well">

  <div class="project-title">
    Controlling and Maximizing the Humanoid Robot Pushing Force by Postures
  </div>
  Y. Jun, <strong>A. Alspach</strong>, P.Y. Oh
  <br>
  Proc. Int'l Conf. on Ubiquitous Robots and Ambient Intelligence (URAI), Daejeon, South Korea, Nov. 2012.
  

  <div id="link-table">
    <table>
      <tr>
        <td class="table-title"><span class="label label-info pull-right"> paper </span></td>
        <td><a target="_blank" href="files/papers/2012-Jun_Alspach-URAI-Controlling_and_Maximizing_the_Humanoid_Robot_Pushing_Force_by_Postures.pdf">Controlling and Maximizing the Humanoid Robot Pushing Force by Postures (URAI 2012)</a></td>
      </tr>  
      <tr>
        <td><span class="label label-info pull-right"> pres </span></td>
        <td><a target="_blank" href="files/presentations/2012-Jun_Alspach-Controlling_and_Maximizing_the_Humanoid_Robot_Pushing_Force_Through_Posture.pdf">Controlling and Maximizing the Humanoid Robot Pushing Force by Postures (URAI 2012)</a></td>
      </tr>            
    </table>
  </div>  
</div>  




<a name="HumanoidRobotPushingThesis" id="HumanoidRobotPushingThesis"></a>    
<div class="well">

  <div class="project-title">
    A Humanoid Robot Pushing Model Inspired by Human Motion
  </div>
  <strong>A. Alspach</strong>
  <br>
  Master's Thesis, Drexel University, Philadelphia, PA, June 2012. (Work published in URAI 2012)
  

  <div id="link-table">
    <table>
      <tr>
        <td class="table-title"><span class="label label-info pull-right">paper</span></td>
        <td><a target="_blank" href="files/papers/2012-Alspach-A_Humanoid_Robot_Pushing_Model_Inspired_by_Human_Motion.pdf">A Humanoid Robot Pushing Model Inspired by Human Motion (2012)</a></td>
      </tr>   
      <tr>
        <td><span class="label label-info pull-right"> poster </span></td>
        <td><a target="_blank" href="files/presentations/2012-Alspach-A_Humanoid_Robot_Pushing_Model_Inspired_by Human_Motion-Masters_Thesis.pdf">A Humanoid Robot Pushing Model Inspired by Human Motion (2012)</a></td>
      </tr>            
    </table>
  </div> 
</div>  




<a name="CommonInterface" id="CommonInterface"></a>     
<div class="well">

  <div class="project-title">
    A Common Interface for Humanoid Simulation and Hardware
  </div>
  R. Ellenberg, R. Sherbert, P.Y. Oh, <strong>A. Alspach</strong>, R. Gross, J.H. Oh
  <br>
  Proc. IEEE-RAS Int'l Conf. Humanoid Robotics, Nashville, TN, Dec. 2010. 
  
  <div id="link-table">
    <table>
      <tr>
        <td class="table-title"><span class="label label-info pull-right">paper</span></td>
        <td><a target="_blank" href="files/papers/2010-Ellenberg_Sherbert-A_Common_Interface_for_Humanoid_Simulation_and_Hardware.pdf">Towards a Common Interface for Humanoid Simulation and Hardware (IEEE-RAS 2010)</a></td>
      </tr>       
    </table>
  </div>
</div>  


